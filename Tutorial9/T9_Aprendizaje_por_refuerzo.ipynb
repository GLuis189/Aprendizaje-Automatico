{"cells":[{"cell_type":"markdown","metadata":{"id":"dmFfJcCKok9J"},"source":["# Q-Learning con FrozenLake-v1 ‚õÑ and Taxi-v3 üöï\n","\n","<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/envs.gif\" alt=\"Environments\"/>\n","\n","###üéÆ Entornos:\n","\n","- [FrozenLake-v1](https://gymnasium.farama.org/environments/toy_text/frozen_lake/)\n","- [Taxi-v3](https://gymnasium.farama.org/environments/toy_text/taxi/)\n","\n","###üìö RL-Library:\n","\n","- Python and NumPy\n","- [Gymnasium](https://gymnasium.farama.org/)"]},{"cell_type":"markdown","metadata":{"id":"qa04YZPwpE16"},"source":["## Un peque√±o resumen de Q-Learning"]},{"cell_type":"markdown","metadata":{"id":"Pon7NOGIpXGB"},"source":["*Q-Learning* **es un algoritmo de aprendizaje por refuerzo que**:\n","\n","- Aprende la funci√≥n-Q, una funci√≥n acci√≥n-valor que codifica una tabla Q que contiene todos los valores de los pares estado-acci√≥n de un problema.\n","\n","\n","- Dado un estado y una acci√≥n, nuestra funci√≥n Q buscar√° en la tabla Q el valor correspondiente\n","    \n","<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-function-2.jpg\" alt=\"Q function\"  width=\"100%\"/>\n","\n","- Una vez finalizado el entrenamiento, tendremos una funci√≥n Q √≥ptima y, por tanto, una tabla Q √≥ptima.\n","    \n","- Y si tenemos una funci√≥n Q √≥ptima, tenemos una pol√≠tica √≥ptima, ya que sabemos para, cada estado, la mejor acci√≥n a tomar.\n","\n","<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/link-value-policy.jpg\" alt=\"Link value policy\"  width=\"100%\"/>\n","\n","\n","Pero, al principio, la Q-Table no es √∫til ya que da un valor arbitrario para cada par estado-acci√≥n (la mayor√≠a de las veces inicializamos la Q-Table con valores 0). Pero, a medida que exploremos el entorno y actualicemos nuestra Q-Table nos dar√° mejores aproximaciones\n","\n","<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/q-learning.jpeg\" alt=\"q-learning.jpeg\" width=\"100%\"/>\n","\n","Aqu√≠ tienes el pseudoc√≥digo para Q-Learning:\n","\n","<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-2.jpg\" alt=\"Q-Learning\" width=\"100%\"/>"]},{"cell_type":"markdown","metadata":{"id":"y71fTl2wrBRv"},"source":["## Instalar dependencias y crear un display virtual üîΩ\n","\n","En el notebook, vamos a necesitar generar un v√≠deo para su visualizaci√≥n. Para ello, con Colab, **necesitamos disponer de una pantalla virtual para renderizar el entorno** (y as√≠ grabar la secuencia de las simulaciones).\n","\n","Tenemos que instalar:\n","\n","- `gymnasium`: Contiene los entornos FrozenLake-v1 ‚õÑ y Taxi-v3 üöï.\n","- `pygame`: Necesarios para el interfaz gr√°fico de FrozenLake-v1 y Taxi-v3.\n","- `numpy`: Necesario para gestionar la Q-table.\n"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"6GdS1K34n64Z"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: gymnasium in c:\\users\\189lu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (0.29.1)\n","Requirement already satisfied: numpy>=1.21.0 in c:\\users\\189lu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from gymnasium) (1.26.4)\n","Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\189lu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from gymnasium) (3.0.0)\n","Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\189lu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from gymnasium) (4.9.0)\n","Requirement already satisfied: farama-notifications>=0.0.1 in c:\\users\\189lu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from gymnasium) (0.0.4)\n","Requirement already satisfied: importlib-metadata>=4.8.0 in c:\\users\\189lu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from gymnasium) (7.0.1)\n","Requirement already satisfied: zipp>=0.5 in c:\\users\\189lu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from importlib-metadata>=4.8.0->gymnasium) (3.17.0)\n","Requirement already satisfied: pygame in c:\\users\\189lu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (2.5.2)\n"]},{"name":"stderr","output_type":"stream","text":["ERROR: Invalid requirement: '#'\n"]}],"source":["!pip install gymnasium\n","!pip install pygame\n","!pip install mediapy # Para reproducir v√≠deos"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"JpTAGHRgxw-n"},"outputs":[{"name":"stderr","output_type":"stream","text":["\"sudo\" no se reconoce como un comando interno o externo,\n","programa o archivo por lotes ejecutable.\n","\"sudo\" no se reconoce como un comando interno o externo,\n","programa o archivo por lotes ejecutable.\n","\"apt\" no se reconoce como un comando interno o externo,\n","programa o archivo por lotes ejecutable.\n"]},{"name":"stdout","output_type":"stream","text":["Collecting pyvirtualdisplay\n","  Downloading PyVirtualDisplay-3.0-py3-none-any.whl.metadata (943 bytes)\n","Downloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\n","Installing collected packages: pyvirtualdisplay\n","Successfully installed pyvirtualdisplay-3.0\n"]}],"source":["!sudo apt-get update\n","!sudo apt-get install -y python3-opengl\n","!apt install ffmpeg xvfb\n","!pip3 install pyvirtualdisplay"]},{"cell_type":"markdown","metadata":{"id":"a5QQwcXzyIOW"},"source":["Para asegurarse de que se utilizan las nuevas bibliotecas instaladas, a veces es necesario reiniciar el runtime del entorno. La siguiente celda forzar√° al runtime a bloquearse, por lo que tendr√° que reconectarse de nuevo. Gracias a este truco, podremos ejecutar el display virtual."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QX-LRK9NyVtE"},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mEl kernel se bloque√≥ al ejecutar c√≥digo en la celda actual o en una celda anterior. \n","\u001b[1;31mRevise el c√≥digo de las celdas para identificar una posible causa del error. \n","\u001b[1;31mHaga clic <a href='https://aka.ms/vscodeJupyterKernelCrash'>aqu√≠</a> para obtener m√°s informaci√≥n. \n","\u001b[1;31mVea Jupyter <a href='command:jupyter.viewOutput'>log</a> para obtener m√°s detalles."]}],"source":["# import os\n","# os.kill(os.getpid(), 9)"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"aE9ixk3BydYE"},"outputs":[{"ename":"FileNotFoundError","evalue":"[WinError 2] El sistema no puede encontrar el archivo especificado","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[2], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Virtual display\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyvirtualdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Display\n\u001b[1;32m----> 4\u001b[0m virtual_display \u001b[38;5;241m=\u001b[39m \u001b[43mDisplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvisible\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1400\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m900\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m virtual_display\u001b[38;5;241m.\u001b[39mstart()\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pyvirtualdisplay\\display.py:54\u001b[0m, in \u001b[0;36mDisplay.__init__\u001b[1;34m(self, backend, visible, size, color_depth, bgcolor, use_xauth, retries, extra_args, manage_global_env, **kwargs)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcls\u001b[39m:\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munknown backend: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend)\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(\n\u001b[0;32m     55\u001b[0m     size\u001b[38;5;241m=\u001b[39msize,\n\u001b[0;32m     56\u001b[0m     color_depth\u001b[38;5;241m=\u001b[39mcolor_depth,\n\u001b[0;32m     57\u001b[0m     bgcolor\u001b[38;5;241m=\u001b[39mbgcolor,\n\u001b[0;32m     58\u001b[0m     retries\u001b[38;5;241m=\u001b[39mretries,\n\u001b[0;32m     59\u001b[0m     use_xauth\u001b[38;5;241m=\u001b[39muse_xauth,\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;66;03m# check_startup=check_startup,\u001b[39;00m\n\u001b[0;32m     61\u001b[0m     extra_args\u001b[38;5;241m=\u001b[39mextra_args,\n\u001b[0;32m     62\u001b[0m     manage_global_env\u001b[38;5;241m=\u001b[39mmanage_global_env,\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m     64\u001b[0m )\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pyvirtualdisplay\\xvfb.py:44\u001b[0m, in \u001b[0;36mXvfbDisplay.__init__\u001b[1;34m(self, size, color_depth, bgcolor, use_xauth, fbdir, dpi, retries, extra_args, manage_global_env)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fbdir \u001b[38;5;241m=\u001b[39m fbdir\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dpi \u001b[38;5;241m=\u001b[39m dpi\n\u001b[1;32m---> 44\u001b[0m \u001b[43mAbstractDisplay\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mPROGRAM\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_xauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_xauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextra_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmanage_global_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanage_global_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pyvirtualdisplay\\abstractdisplay.py:85\u001b[0m, in \u001b[0;36mAbstractDisplay.__init__\u001b[1;34m(self, program, use_xauth, retries, extra_args, manage_global_env)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pipe_wfd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retries_current \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 85\u001b[0m helptext \u001b[38;5;241m=\u001b[39m \u001b[43mget_helptext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprogram\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_displayfd \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-displayfd\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m helptext\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_displayfd:\n","File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\pyvirtualdisplay\\util.py:13\u001b[0m, in \u001b[0;36mget_helptext\u001b[1;34m(program)\u001b[0m\n\u001b[0;32m      6\u001b[0m cmd \u001b[38;5;241m=\u001b[39m [program, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-help\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# py3.7+\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# p = subprocess.run(cmd, capture_output=True)\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# stderr = p.stderr\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# py3.6 also\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPIPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstderr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPIPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshell\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m _, stderr \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mcommunicate()\n\u001b[0;32m     21\u001b[0m helptext \u001b[38;5;241m=\u001b[39m stderr\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_3.9.3568.0_x64__qbz5n2kfra8p0\\lib\\subprocess.py:951\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask)\u001b[0m\n\u001b[0;32m    947\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_mode:\n\u001b[0;32m    948\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[0;32m    949\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m--> 951\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    952\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    953\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    954\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    955\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    956\u001b[0m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    957\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    958\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mgid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mumask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    959\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m    961\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n\u001b[0;32m    962\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdin, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr)):\n","File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_3.9.3568.0_x64__qbz5n2kfra8p0\\lib\\subprocess.py:1420\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_gid, unused_gids, unused_uid, unused_umask, unused_start_new_session)\u001b[0m\n\u001b[0;32m   1418\u001b[0m \u001b[38;5;66;03m# Start the process\u001b[39;00m\n\u001b[0;32m   1419\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1420\u001b[0m     hp, ht, pid, tid \u001b[38;5;241m=\u001b[39m \u001b[43m_winapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCreateProcess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1421\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;66;43;03m# no special security\u001b[39;49;00m\n\u001b[0;32m   1422\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1423\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1424\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1425\u001b[0m \u001b[43m                             \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1426\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1427\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1428\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1429\u001b[0m     \u001b[38;5;66;03m# Child is launched. Close the parent's copy of those pipe\u001b[39;00m\n\u001b[0;32m   1430\u001b[0m     \u001b[38;5;66;03m# handles that only the child should have open.  You need\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1433\u001b[0m     \u001b[38;5;66;03m# pipe will not close when the child process exits and the\u001b[39;00m\n\u001b[0;32m   1434\u001b[0m     \u001b[38;5;66;03m# ReadFile will hang.\u001b[39;00m\n\u001b[0;32m   1435\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_pipe_fds(p2cread, p2cwrite,\n\u001b[0;32m   1436\u001b[0m                          c2pread, c2pwrite,\n\u001b[0;32m   1437\u001b[0m                          errread, errwrite)\n","\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] El sistema no puede encontrar el archivo especificado"]}],"source":["# Virtual display\n","from pyvirtualdisplay import Display\n","\n","virtual_display = Display(visible=0, size=(1400, 900))\n","virtual_display.start()"]},{"cell_type":"markdown","metadata":{"id":"2-wSZbdFyjrv"},"source":["## Importamos los paquetes üì¶\n","\n","In addition to the installed libraries, we also use:\n","\n","- `random`: generar n√∫meros aleatorios (ser√° √∫tilo para la pol√≠tica epsilon-greedy).\n","- `imageio`: Manejar el v√≠deo que hagamos."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gOXTFuoezC-v"},"outputs":[],"source":["import numpy as np\n","import gymnasium as gym\n","import random\n","import imageio\n","import os\n","import tqdm\n","\n","from tqdm.notebook import tqdm"]},{"cell_type":"markdown","metadata":{"id":"lWER47z8zSJl"},"source":["# Parte 1: Frozen Lake ‚õÑ (versi√≥n no resbaladiza)"]},{"cell_type":"markdown","metadata":{"id":"1KqrQ0FYz0mQ"},"source":["## Entendiendo [Entorno FrozenLake ‚õÑ]((https://gymnasium.farama.org/environments/toy_text/frozen_lake/)\n","---\n","\n","üí° Una buena costumbre cuando empiezas a usar un entorno es consultar su documentaci√≥n\n","\n","üëâ https://gymnasium.farama.org/environments/toy_text/frozen_lake/\n","\n","---\n","\n","Vamos a entrenar a nuestro agente Q-Learning **para que navegue desde el estado inicial (S) hasta el estado meta (G) caminando s√≥lo sobre baldosas congeladas (F) y evitando los agujeros (H)**.\n","\n","Podemos tener dos tama√±os de entorno:\n","\n","- `map_name=\"4x4\"`: una versi√≥n de cuadr√≠cula 4x4\n","- `map_name=\"8x8\"`: una versi√≥n en cuadr√≠cula de 8x8\n","\n","\n","El entorno tiene dos modos:\n","\n","- `is_slippery=False`: El agente siempre se mueve **en la direcci√≥n prevista** debido a la naturaleza no resbaladiza del lago helado (determinista).\n","- `is_slippery=True`: El agente **puede no moverse siempre en la direcci√≥n prevista** debido a la naturaleza resbaladiza del lago helado (estoc√°stico)."]},{"cell_type":"markdown","metadata":{"id":"ufvri27f0YcS"},"source":["### Soluci√≥n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DN-qyT4q0e7-"},"outputs":[],"source":["env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False, render_mode=\"rgb_array\")"]},{"cell_type":"markdown","metadata":{"id":"T3VqFWdz0pKg"},"source":["Podemos crear nuestro propio grid como se muestra a continuaci√≥n:\n","\n","```python\n","desc=[\"SFFF\", \"FHFH\", \"FFFH\", \"HFFG\"]\n","gym.make('FrozenLake-v1', desc=desc, is_slippery=False)\n","```\n","\n","por ahora, cogemos el entorno por defecto.\n"]},{"cell_type":"markdown","metadata":{"id":"QiJHdQaf0-Jt"},"source":["### Veamos c√≥mo es el entorno:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gkRvDwlu1Fwm"},"outputs":[],"source":["# We create our environment with gym.make(\"<name_of_the_environment>\")- `is_slippery=False`: The agent always moves in the intended direction due to the non-slippery nature of the frozen lake (deterministic).\n","print(\"_____OBSERVATION SPACE_____ \\n\")\n","print(\"Observation Space\", env.observation_space)\n","print(\"Sample observation\", env.observation_space.sample()) # Get a random observation"]},{"cell_type":"markdown","metadata":{"id":"zTdf0rUQ1i3T"},"source":["`Observation Space Shape Discrete(16)` indica que la observaci√≥n es un entero que representa la **posici√≥n actual del agente como fila_actual * ncols + col_actual (donde tanto la fila como la col empiezan en 0)**.\n","\n","Por ejemplo, la posici√≥n de la meta en el mapa 4x4 se puede calcular de la siguiente manera: 3 * 4 + 3 = 15. El n√∫mero de observaciones posibles depende del tama√±o del mapa. **Por ejemplo, el mapa 4x4 tiene 16 observaciones posibles**.\n","\n","Por ejemplo, esto es lo que representar√≠a con `state = 0`:\n","\n","<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/frozenlake.png\" alt=\"FrozenLake\">\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1kxI48SN2L3E"},"outputs":[],"source":["print(\"\\n _____ACTION SPACE_____ \\n\")\n","print(\"Action Space Shape\", env.action_space.n)\n","print(\"Action Space Sample\", env.action_space.sample()) # Take a random action"]},{"cell_type":"markdown","metadata":{"id":"sfZ0vSpv2fqR"},"source":["El espacio de acciones (el conjunto de acciones posibles que puede realizar el agente) es discreto con 4 acciones disponibles üéÆ:\n","- 0: IR A LA IZQUIERDA\n","- 1: IR ABAJO\n","- 2: IR A LA DERECHA\n","- 3: IR ARRIBA\n","\n","Funci√≥n de recompensa üí∞:\n","- Llegar a la meta: +1\n","- Caer en un agujero: 0\n","- Quedarte congelado: 0"]},{"cell_type":"markdown","metadata":{"id":"BxuaaQGz26j9"},"source":["## Crear e inicializar la tabla Q üóÑÔ∏è\n","\n","(üëÄ Paso 1 del pseudoc√≥digo)\n","\n","<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-2.jpg\" alt=\"Q-Aprendizaje\" width=\"100%\"/>\n","\n","\n","Para saber cu√°ntas filas (estados) y columnas (acciones) a utilizar, necesitamos saber las acciones y el n√∫mero de estados del problema. Ya conocemos sus valores de antes, pero vamos a querer obtenerlos para que el algoritmo se generalice para diferentes entornos. Gym nos proporciona una forma de hacerlo: `env.espacio_acci√≥n.n` y `env.espacio_observacion.n`.\n"]},{"cell_type":"markdown","metadata":{"id":"6t-D4Asb3WuJ"},"source":["### Soluci√≥n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"we_JTDCK2eZc"},"outputs":[],"source":["state_space = env.observation_space.n\n","print(\"There are \", state_space, \" possible states\")\n","\n","action_space = env.action_space.n\n","print(\"There are \", action_space, \" possible actions\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jpeJ8dnJ3jOD"},"outputs":[],"source":["# Let's create our Qtable of size (state_space, action_space) and initialized each values at 0 using np.zeros\n","def initialize_q_table(state_space, action_space):\n","  Qtable = np.zeros((state_space, action_space))\n","  return Qtable"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s8w6e18z3p_8"},"outputs":[],"source":["Qtable_frozenlake = initialize_q_table(state_space, action_space)"]},{"cell_type":"markdown","metadata":{"id":"ySAhxGpu3w-7"},"source":["## Definir la mejor pol√≠tica ü§ñ\n","\n","Recuerda que tenemos dos pol√≠ticas ya que Q-Learning es un algoritmo **off-policy**. Esto significa que estamos usando una **pol√≠tica diferente para actuar y actualizar la funci√≥n de valor**.\n","\n","- Pol√≠tica epsilon-greedy (exploraci√≥n)\n","- Pol√≠tica greedy (explotaci√≥n)\n","\n","La pol√≠tica greedy tambi√©n ser√° la pol√≠tica final que tendremos cuando el agente Q-learning complete el entrenamiento. La pol√≠tica greedy se utiliza para seleccionar una acci√≥n utilizando la tabla Q.\n","\n","<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/off-on-4.jpg\" alt=\"Q-Learning\" width=\"100%\"/>\n"]},{"cell_type":"markdown","metadata":{"id":"DfrwYX6V40XT"},"source":["#### Soluci√≥n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1UEuCyvQ9YgY"},"outputs":[],"source":["def greedy_policy(Qtable, state):\n","  # Exploitation: take the action with the highest state, action value\n","  action = np.argmax(Qtable[state][:])\n","\n","  return action"]},{"cell_type":"markdown","metadata":{"id":"lvP4Plr79h9A"},"source":["## Definici√≥n de la pol√≠tica epsilon-greedy ü§ñ"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c_ftCUbn4-t1"},"outputs":[],"source":["def epsilon_greedy_policy(Qtable, state, epsilon):\n","  # Randomly generate a number between 0 and 1\n","  random_num = random.uniform(0,1)\n","  # if random_num > greater than epsilon --> exploitation\n","  if random_num > epsilon:\n","    # Take the action with the highest value given a state\n","    # np.argmax can be useful here\n","    action = greedy_policy(Qtable, state)\n","  # else --> exploration\n","  else:\n","    action = env.action_space.sample()\n","\n","  return action"]},{"cell_type":"markdown","metadata":{"id":"4Kwh7T_t5K2e"},"source":["## Definici√≥n de hiperpar√°metros ‚öôÔ∏è\n","\n","Los hiperpar√°metros relacionados con la exploraci√≥n son algunos de los m√°s importantes.\n","\n","- Tenemos que asegurarnos de que nuestro agente **explora lo suficiente del espacio de estados** para aprender una buena aproximaci√≥n de valores. Para ello, necesitamos tener un decaimiento progresivo del √©psilon.\n","- Si disminuimos epsilon demasiado r√°pido (tasa de decaimiento demasiado alta), **corremos el riesgo de que nuestro agente se quede atascado**, ya que no ha explorado lo suficiente el espacio de estados y, por tanto, no puede resolver el problema.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rvu4NzGI5ziu"},"outputs":[],"source":["# Training parameters\n","n_training_episodes = 10000  # Total training episodes\n","learning_rate = 0.7          # Learning rate\n","\n","# Evaluation parameters\n","n_eval_episodes = 100        # Total number of test episodes\n","\n","# Environment parameters\n","env_id = \"FrozenLake-v1\"     # Name of the environment\n","max_steps = 99               # Max steps per episode\n","gamma = 0.95                 # Discounting rate\n","eval_seed = []               # The evaluation seed of the environment\n","\n","# Exploration parameters\n","max_epsilon = 1.0             # Exploration probability at start\n","min_epsilon = 0.05            # Minimum exploration probability\n","decay_rate = 0.0005            # Exponential decay rate for exploration prob"]},{"cell_type":"markdown","metadata":{"id":"jIqDCeIw6FYi"},"source":["## El bucle de entrenamiento\n","\n","<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-2.jpg\" alt=\"Q-Learning\" width=\"100%\"/>\n","\n","El bucle de entrenamiento ser√≠a como lo siguiente:\n","\n","```\n","Por cada episodio:\n","\n","Reducimos epsilon (ya que cada vez se necesita menos exploraci√≥n)\n","Restablecer el entorno\n","\n","  Para cada paso dentro del n√∫mero m√°ximo (`max_steps`):\n","    Eleginos la acci√≥n At utilizando la pol√≠tica epsilon-greedy\n","    Realiza la acci√≥n (a) y observa el estado resultante (s') y la recompensa (r)\n","    Actualizar el Q-valor Q(s,a) utilizando la ecuaci√≥n de Bellman Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n","    Si hemos llegado a un estado final, terminar el episodio\n","    El siguiente estado es el nuevo estado s'\n","```"]},{"cell_type":"markdown","metadata":{"id":"vNGNwqYO7-01"},"source":["#### Soluci√≥n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rgbFI2g18OKS"},"outputs":[],"source":["def train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable):\n","  for episode in tqdm(range(n_training_episodes)):\n","    # Reduce epsilon (because we need less and less exploration)\n","    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n","    # Reset the environment\n","    state, info = env.reset()\n","    step = 0\n","    terminated = False\n","    truncated = False\n","\n","    # repeat\n","    for step in range(max_steps):\n","      # Choose the action At using epsilon greedy policy\n","      action = epsilon_greedy_policy(Qtable, state, epsilon)\n","\n","      # Take action At and observe Rt+1 and St+1\n","      # Take the action (a) and observe the outcome state(s') and reward (r)\n","      new_state, reward, terminated, truncated, info = env.step(action)\n","\n","      # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n","      Qtable[state][action] = Qtable[state][action] + learning_rate * (reward + gamma * np.max(Qtable[new_state]) - Qtable[state][action])\n","\n","      # If terminated or truncated finish the episode\n","      if terminated or truncated:\n","        break\n","\n","      # Our next state is the new state\n","      state = new_state\n","  return Qtable"]},{"cell_type":"markdown","metadata":{"id":"GPVPkrKY8YG0"},"source":["## Entrenando el agente Q-Learning üèÉ"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ky5bTfKL8fgm"},"outputs":[],"source":["Qtable_frozenlake = train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable_frozenlake)"]},{"cell_type":"markdown","metadata":{"id":"p3zoo9Rn94iQ"},"source":["## Veamos como queda la Q-tabla üëÄ"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e26pU4UU-C1t"},"outputs":[],"source":["Qtable_frozenlake"]},{"cell_type":"markdown","metadata":{"id":"XFFel7wJ-iAD"},"source":["## El m√©todo de evaluaci√≥n üìù\n","\n","- Definimos el m√©todo de evaluaci√≥n que vamos a utilizar para probar nuestro agente Q-Learning."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cP-cnFja-uU_"},"outputs":[],"source":["def evaluate_agent(env, max_steps, n_eval_episodes, Q, seed):\n","  \"\"\"\n","  Evaluate the agent for ``n_eval_episodes`` episodes and returns average reward and std of reward.\n","  :param env: The evaluation environment\n","  :param max_steps: Maximum number of steps per episode\n","  :param n_eval_episodes: Number of episode to evaluate the agent\n","  :param Q: The Q-table\n","  :param seed: The evaluation seed array (for taxi-v3)\n","  \"\"\"\n","  episode_rewards = []\n","  for episode in tqdm(range(n_eval_episodes)):\n","    if seed:\n","      state, info = env.reset(seed=seed[episode])\n","    else:\n","      state, info = env.reset()\n","    step = 0\n","    truncated = False\n","    terminated = False\n","    total_rewards_ep = 0\n","\n","    for step in range(max_steps):\n","      # Take the action (index) that have the maximum expected future reward given that state\n","      action = greedy_policy(Q, state)\n","      new_state, reward, terminated, truncated, info = env.step(action)\n","      total_rewards_ep += reward\n","\n","      if terminated or truncated:\n","        break\n","      state = new_state\n","    episode_rewards.append(total_rewards_ep)\n","  mean_reward = np.mean(episode_rewards)\n","  std_reward = np.std(episode_rewards)\n","\n","  return mean_reward, std_reward"]},{"cell_type":"markdown","metadata":{"id":"YWZQTAEh--VO"},"source":["## Evaluar nuestro agente Q-Learning üìà\n","\n","- Normalmente, deber√≠a tener una recompensa media de 1.0\n","- El **entorno es relativamente f√°cil** ya que el espacio de estados es realmente peque√±o (16). Lo que puedes intentar es [sustituirlo por la versi√≥n resbaladiza](https://gymnasium.farama.org/environments/toy_text/frozen_lake/), que introduce estocasticidad, haciendo el entorno m√°s complejo."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rffmPRI7_Klf"},"outputs":[],"source":["# Evaluate our Agent\n","mean_reward, std_reward = evaluate_agent(env, max_steps, n_eval_episodes, Qtable_frozenlake, eval_seed)\n","print(f\"Mean_reward={mean_reward:.2f} +/- {std_reward:.2f}\")"]},{"cell_type":"markdown","metadata":{"id":"QuHp3BYU_RsP"},"source":["#### No modificar este c√≥digo"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qdi5-D33_YGy"},"outputs":[],"source":["def record_video(env, Qtable, out_directory, fps=1, max_iter=100):\n","  \"\"\"\n","  Generate a replay video of the agent\n","  :param env\n","  :param Qtable: Qtable of our agent\n","  :param out_directory\n","  :param fps: how many frame per seconds (with taxi-v3 and frozenlake-v1 we use 1)\n","  \"\"\"\n","  images = []\n","  terminated = False\n","  truncated = False\n","  state, info = env.reset(seed=random.randint(0,500))\n","  img = env.render()\n","  images.append(img)\n","  iters = 0\n","  while iters<max_iter and (not terminated or truncated):\n","    # Take the action (index) that have the maximum expected future reward given that state\n","    action = np.argmax(Qtable[state][:])\n","    state, reward, terminated, truncated, info = env.step(action) # We directly put next_state = state for recording logic\n","    img = env.render()\n","    images.append(img)\n","    iters = iters+1\n","  imageio.mimsave(out_directory, [np.array(img) for i, img in enumerate(images)], fps=fps)"]},{"cell_type":"markdown","metadata":{"id":"zA-K8oN6_kOc"},"source":["- Vamos a crear **el diccionario del modelo que contiene los hiperpar√°metros y la tabla Q**."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5YR5oyPS_r7e"},"outputs":[],"source":["model = {\n","    \"env_id\": env_id,\n","    \"max_steps\": max_steps,\n","    \"n_training_episodes\": n_training_episodes,\n","    \"n_eval_episodes\": n_eval_episodes,\n","    \"eval_seed\": eval_seed,\n","\n","    \"learning_rate\": learning_rate,\n","    \"gamma\": gamma,\n","\n","    \"max_epsilon\": max_epsilon,\n","    \"min_epsilon\": min_epsilon,\n","    \"decay_rate\": decay_rate,\n","\n","    \"qtable\": Qtable_frozenlake\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o5lDqT51_8lP"},"outputs":[],"source":["evaluate_agent(env, model[\"max_steps\"], model[\"n_eval_episodes\"], model[\"qtable\"], model[\"eval_seed\"])\n","\n","readme_path = \".\"\n","# Step 6: Record a video\n","video_path = \"replay.mp4\"\n","record_video(env, model[\"qtable\"], video_path, 10)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q66nEsMEAJ11"},"outputs":[],"source":["import mediapy as media\n","video = media.read_video('replay.mp4')\n","media.show_video(video)"]},{"cell_type":"markdown","metadata":{"id":"_PSODRL_NG1T"},"source":["# Parte 2: Taxi-v3 üöñ\n","\n","## Entendiendo [Taxi-v3 üöï](https://gymnasium.farama.org/environments/toy_text/taxi/)\n","---\n","\n","üí° Una buena costumbre cuando se empieza a utilizar un entorno es consultar su documentaci√≥n\n","\n","üëâ https://gymnasium.farama.org/environments/toy_text/taxi/\n","\n","---\n","En `Taxi-v3` üöï, hay cuatro lugares designados en el mundo cuadriculado indicados por R(ed), G(reen), Y(ellow) y B(lue).\n","\n","Cuando comienza el episodio, **el taxi parte de una casilla aleatoria** y el pasajero se encuentra en un lugar aleatorio. El taxi se dirige al lugar donde se encuentra el pasajero, **recoge al pasajero**, se dirige al destino del pasajero (otro de los cuatro lugares especificados) y **deja al pasajero**. Una vez que se deja al pasajero, el episodio termina.\n","\n","<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/taxi.png\" alt=\"Taxi\">\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oi_fhYHzNpK8"},"outputs":[],"source":["env = gym.make(\"Taxi-v3\", render_mode=\"rgb_array\")"]},{"cell_type":"markdown","metadata":{"id":"Q_o3CMzMN2ld"},"source":["Hay **500 estados discretos, ya que hay 25 posiciones del taxi, 5 posibles ubicaciones del pasajero** (incluido el caso en que el pasajero est√° en el taxi) y **4 ubicaciones de destino.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ByWrm0qGONcx"},"outputs":[],"source":["state_space = env.observation_space.n\n","print(\"There are \", state_space, \" possible states\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nxcts4AvOVh9"},"outputs":[],"source":["action_space = env.action_space.n\n","print(\"There are \", action_space, \" possible actions\")"]},{"cell_type":"markdown","metadata":{"id":"9a9cCKhgOpvD"},"source":["El espacio de acciones (el conjunto de acciones posibles que puede realizar el agente) es discreto con **6 acciones disponibles üéÆ**:\n","\n","- 0: desplazarse al sur\n","- 1: hacia el norte\n","- 2: desplazarse al este\n","- 3: desplazarse al oeste\n","- 4: recoger pasajero\n","- 5: dejar pasajero\n","\n","Funci√≥n de recompensa üí∞:\n","\n","- -1 por paso, a menos que se active otra recompensa.\n","- +20 por entregar pasajero.\n","- -10 ejecutando acciones \"recoger\" y \"dejar\" ilegalmente."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z0Ifpk2fPP_U"},"outputs":[],"source":["# Create our Q table with state_size rows and action_size columns (500x6)\n","Qtable_taxi = initialize_q_table(state_space, action_space)\n","print(Qtable_taxi)\n","print(\"Q-table shape: \", Qtable_taxi .shape)"]},{"cell_type":"markdown","metadata":{"id":"n6gV9OrtPzjo"},"source":["## Definir los hiperpar√°metros ‚öôÔ∏è\n","\n","‚ö† NO MODIFICAR EVAL_SEED: el array eval_seed **nos permite evaluar un agente con las mismas posiciones de partida de taxi para cada ejecuci√≥n**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zMXoR1uuP_BJ"},"outputs":[],"source":["# Training parameters\n","n_training_episodes = 25000   # Total training episodes\n","learning_rate = 0.7           # Learning rate\n","\n","# Evaluation parameters\n","n_eval_episodes = 100        # Total number of test episodes\n","\n","# DO NOT MODIFY EVAL_SEED\n","eval_seed = [16,54,165,177,191,191,120,80,149,178,48,38,6,125,174,73,50,172,100,148,146,6,25,40,68,148,49,167,9,97,164,176,61,7,54,55,\n"," 161,131,184,51,170,12,120,113,95,126,51,98,36,135,54,82,45,95,89,59,95,124,9,113,58,85,51,134,121,169,105,21,30,11,50,65,12,43,82,145,152,97,106,55,31,85,38,\n"," 112,102,168,123,97,21,83,158,26,80,63,5,81,32,11,28,148] # Evaluation seed, this ensures that all classmates agents are trained on the same taxi starting position\n","                                                          # Each seed has a specific starting state\n","\n","# Environment parameters\n","env_id = \"Taxi-v3\"           # Name of the environment\n","max_steps = 99               # Max steps per episode\n","gamma = 0.95                 # Discounting rate\n","\n","# Exploration parameters\n","max_epsilon = 1.0             # Exploration probability at start\n","min_epsilon = 0.05           # Minimum exploration probability\n","decay_rate = 0.005            # Exponential decay rate for exploration prob"]},{"cell_type":"markdown","metadata":{"id":"LUcFHgXbQHa8"},"source":["## Entrenamos el agente Q-Learning"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QQ39V4W_QO3N"},"outputs":[],"source":["Qtable_taxi = train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable_taxi)\n","Qtable_taxi"]},{"cell_type":"markdown","metadata":{"id":"dxi346mKQoHn"},"source":["Evaluamos el agente Q-Learning"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"48YZQKuCQrVi"},"outputs":[],"source":["# Evaluate our Agent\n","mean_reward_taxi, std_reward_taxi = evaluate_agent(env, max_steps, n_eval_episodes, Qtable_taxi, eval_seed)\n","print(f\"Mean_reward={mean_reward_taxi:.2f} +/- {std_reward_taxi:.2f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UQU37KVDRba4"},"outputs":[],"source":["model = {\n","    \"env_id\": env_id,\n","    \"max_steps\": max_steps,\n","    \"n_training_episodes\": n_training_episodes,\n","    \"n_eval_episodes\": n_eval_episodes,\n","    \"eval_seed\": eval_seed,\n","\n","    \"learning_rate\": learning_rate,\n","    \"gamma\": gamma,\n","\n","    \"max_epsilon\": max_epsilon,\n","    \"min_epsilon\": min_epsilon,\n","    \"decay_rate\": decay_rate,\n","\n","    \"qtable\": Qtable_taxi\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3_wVoqWeRoCq"},"outputs":[],"source":["#evaluate_agent(env, model[\"max_steps\"], model[\"n_eval_episodes\"], model[\"qtable\"], model[\"eval_seed\"])\n","\n","readme_path = \".\"\n","# Step 6: Record a video\n","video_path = \"replay_taxi.mp4\"\n","record_video(env, model[\"qtable\"], video_path, 2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VeQrP2-uRtHK"},"outputs":[],"source":["import mediapy as media\n","video = media.read_video('replay_taxi.mp4')\n","media.show_video(video)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":0}
